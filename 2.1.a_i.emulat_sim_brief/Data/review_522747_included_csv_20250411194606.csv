Title,Authors,Abstract,Published Year,Published Month,Journal,Volume,Issue,Pages,Accession Number,DOI,Ref,Covidence #,Study,Notes,Tags
Imputing missing covariates in time-to-event analysis within distributed research networks: A simulation study.,"Dongdong Li, Jenna Wong, Xiaojuan Li, Sengwee Toh, Rui Wang","In distributed research network (DRN) settings, multiple imputation cannot be directly implemented because pooling individual-level data are often not feasible. The performance of multiple imputation in combination with meta-analysis is not well understood within DRNs. To evaluate the performance of imputation for missing baseline covariate data in combination with meta-analysis for time-to-event analysis within DRNs, we compared two parametric algorithms including one approximated linear imputation model (Approx), and one nonlinear substantive model compatible imputation model (SMC), as well as two non-parametric machine learning algorithms including random forest (RF), and classification and regression trees (CART), through simulation studies motivated by a real-world data set. Under the setting with small effect sizes (i.e., log-Hazard ratios [logHR]) and homogeneous missingness mechanisms across sites, all imputation methods produced unbiased and more efficient estimates while the complete-case analysis could be biased and inefficient; and under heterogeneous missingness mechanisms, estimates with RF method could have higher efficiency. Estimates from the distributed imputation combined by meta-analysis were similar to those from the imputation using pooled data. When logHRs were large, the SMC imputation algorithm generally performed better than others. These findings suggest the validity and feasibility of imputation within DRNs in the presence of missing covariate data in time-to-event analysis under various settings. The performance of the four imputation algorithms varies with the effect sizes and level of missingness.",2023,,Pharmacoepidemiology and drug safety,,,,36380400,10.1002/pds.5563,,#1791,DongdongLi 2023,"",""
Exploring the impact of design criteria for reference sets on performance evaluation of signal detection algorithms: The case of drug-drug interactions.,"Elpida Kontsioti, Simon Maskell, Munir Pirmohamed","To evaluate the impact of multiple design criteria for reference sets that are used to quantitatively assess the performance of pharmacovigilance signal detection algorithms (SDAs) for drug-drug interactions (DDIs). Starting from a large and diversified reference set for two-way DDIs, we generated custom-made reference sets of various sizes considering multiple design criteria (e.g., adverse event background prevalence). We assessed differences observed in the performance metrics of three SDAs when applied to FDA Adverse Event Reporting System (FAERS) data. For some design criteria, the impact on the performance metrics was neglectable for the different SDAs (e.g., theoretical evidence associated with positive controls), while others (e.g., restriction to designated medical events, event background prevalence) seemed to have opposing and effects of different sizes on the Area Under the Curve (AUC) and positive predictive value (PPV) estimates. The relative composition of reference sets can significantly impact the evaluation metrics, potentially altering the conclusions regarding which methodologies are perceived to perform best. We therefore need to carefully consider the selection of controls to avoid misinterpretation of signals triggered by confounding factors rather than true associations as well as adding biases to our evaluation by ""favoring"" some algorithms while penalizing others.",2023,,Pharmacoepidemiology and drug safety,,,,36916014,10.1002/pds.5609,,#1831,ElpidaKontsioti 2023,"",""
Bias amplification of unobserved confounding in pharmacoepidemiological studies using indication-based sampling.,"Viktor H Ahlqvist, Paul Madley-Dowd, Amanda Ly, Jessica Rast, Michael Lundberg, Egill J&#xf3; nsson-Bachmann, Daniel Berglind, Dheeraj Rai, Cecilia Magnusson, Brian K Lee","Estimating causal effects in observational pharmacoepidemiology is a challenging task, as it is often plagued by confounding by indication. Restricting the sample to those with an indication for drug use is a commonly performed procedure; indication-based sampling ensures that the exposed and unexposed are exchangeable on the indication-limiting the potential for confounding by indication. However, indication-based sampling has received little scrutiny, despite the hazards of exposure-related covariate control. Using simulations of varying levels of confounding and applied examples we describe bias amplification under indication-based sampling. We demonstrate that indication-based sampling in the presence of unobserved confounding can give rise to bias amplification, a self-inflicted phenomenon where one inflates pre-existing bias through inappropriate covariate control. Additionally, we show that indication-based sampling generally leads to a greater net bias than alternative approaches, such as regression adjustment. Finally, we expand on how bias amplification should be reasoned about when distinct clinically relevant effects on the outcome among those with an indication exist (effect-heterogeneity). We conclude that studies using indication-based sampling should have robust justification - and that it should by no means be considered unbiased to adopt such approaches. As such, we suggest that future observational studies stay wary of bias amplification when considering drug indications.",2023,,Pharmacoepidemiology and drug safety,,,,36919941,10.1002/pds.5614,,#1834,ViktorHAhlqvist 2023,"",""
Sensitivity and specificity in signal detection with the reporting odds ratio and the information component.,"Peter Trillenberg, Andreas Sprenger, Bj&#xf6; rn Machner","As measures of association between an adverse drug reaction (ADR) and exposure to a drug the reporting odds ratio (ROR) and the information component (IC) can be used. We sought to test the reliability of signal detection with these. We simulated ADR counts as binomially distributed random numbers for different expected ADR frequencies and theoretical reporting odds ratios (RORs). We then calculated the empirical IC and the empirical ROR and their confidence intervals. The rate of signals that was detected despite a theoretical ROR of 1 represented the false positive rate, and represented the sensitivity if the ROR was &gt;1. For expected case counts below 1 the false positive rate oscillates from 0.01 to 0.1 even though 0.025 were intended. Even beyond expected case counts of 5 oscillations can cover a range of 0.018 to 0.035. The first n oscillations with the largest amplitude are eliminated if a minimum case count of n is required. To detect an ROR of 2 with a sensitivity of 0.8, a minimum of 12 expected ADRs are required. In contrast, 2 expected ADRs suffice to detect an ROR of 4. Summaries of measures for disproportionality should include the expected number of cases in the group of interest if a signal was detected. If no signal was detected the sensitivity for the detection of a representative ROR or the minimum ROR that could be detected with probability 0.8 should be reported.",2023,,Pharmacoepidemiology and drug safety,,,,36966482,10.1002/pds.5624,,#1846,PeterTrillenberg 2023,"",""
Differences in target estimands between different propensity score-based weights.,Peter C Austin,"Propensity score weighting is a popular approach for estimating treatment effects using observational data. Different sets of propensity score-based weights have been proposed, including inverse probability of treatment weights whose target estimand is the average treatment effect, weights whose target estimand is the average treatment effect in the treated (ATT), and, more recently, matching weights, overlap weights, and entropy weights. These latter three sets of weights focus on estimating the effect of treatment in those subjects for whom there is clinical equipoise. We conducted a series of simulations to explore differences in the value of the target estimands for these five sets of weights when the difference in means is the measure of treatment effect. We considered 648 scenarios defined by different values of the prevalence of treatment, the c-statistic of the propensity score model, the correlation between the linear predictors for treatment selection and the outcome, and by the magnitude of the interaction between treatment status and the linear predictor for the outcome in the absence of treatment. We found that, when the prevalence of treatment was low or high and the c-statistic of the propensity score model was moderate to high, that matching weights, overlap weights, and entropy weights had target estimands that differed meaningfully from the target estimand of the ATE weights. Researchers using matching weights, overlap weights, and entropy weights should not assume that the estimated treatment effect is comparable to the ATE.",2023,,Pharmacoepidemiology and drug safety,,,,37208837,10.1002/pds.5639,,#1860,PeterCAustin 2023,"",""
Probabilistic precision calculations for the planning of studies assessing negative binomial rates.,"Jonatan Hedberg, Stefan Franz&#xe9; n","Outcome variables that are assumed to follow a negative binomial distribution are frequently used in both clinical and epidemiological studies. Epidemiological studies, particularly those performed by pharmaceutical companies often aim to describe a population rather than compare treatments. Such descriptive studies are often analysed using confidence intervals. While precision calculations and sample size calculations are not always performed in these settings, they have the important role of setting expectations of what results the study may generate. Current methods for precision calculations for the negative binomial rate are based on plugging in parameter values into the confidence interval formulae. This method has the downside of ignoring the randomness of the confidence interval limits. To enable better practice for precision calculations, methods are needed that address the randomness. Using the well-known delta-method we develop a method for calculating the precision probability, that is, the probability of achieving a certain width. We assess the performance of the method in smaller samples through simulations. The method for the precision probability performs well in small to medium sample sizes, and the usefulness of the method is demonstrated through an example. We have developed a simple method for calculating the precision probability for negative binomial rates. This method can be used when planning epidemiological studies in for example, asthma, while correctly taking the randomness of confidence intervals into account.",2024,,Pharmacoepidemiology and drug safety,,,,38362649,10.1002/pds.5750,,#1973,JonatanHedberg 2024,"",""
Is the sequence ratio an unbiased estimate of the incidence rate ratio? A simulation study.,"Thomas Delvin, Sofie Egsgaard, Jesper Hallas, Helene Kildegaard, Lars Christian Lund, Martin Torp Rahbek","We aimed to evaluate the conditions under which the sequence ratio (SR) obtained from a sequence symmetry analysis is an unbiased estimate of the true incidence rate ratio (IRR). We simulated cohorts of 1 million individuals who could initiate an exposure drug and experience a very rare, rare, common, or frequent outcome of interest. The outcome rate among exposed individuals was modified by a true incidence rate ratio of 0.2, 0.5, 1.0, 2.0, and 5.0. We further evaluated scenarios where the outcome was fatal and led to immediate censoring or the outcome reduced the rate of initiation of the exposure drug. We found the SR to be close to unbiased for rare, common, and frequent events, except when the true IRR was 5.0 (mean SR 4.94 and 3.74 for common and frequent events). The SR was slightly biased when the outcome was very rare. When the outcome was potentially fatal, the SR was increasingly biased with an increasing probability of death. Likewise, when the outcome reduced the probability of future exposure, the SR was upwards biased. The SR is a biased estimate of the incidence rate ratio, when the true IRR is high, the outcome has a high mortality, or when the outcome reduces the probability of future exposure.",2024,,Pharmacoepidemiology and drug safety,,,,38450934,10.1002/pds.5774,,#1987,ThomasDelvin 2024,"",""
Use of quantitative bias analysis to evaluate single-arm trials with real-world data external controls.,"Christen Gray, Eleanor Ralphs, Matthew P Fox, Timothy L Lash, Geoffrey Liu, Tzuyung Doug Kou, Donna R Rivera, Jaclyn Bosco, Kim Van Naarden Braun,  ..., Deborah Layton","Use of real-world data (RWD) for external controls added to single-arm trials (SAT) is increasingly prevalent in regulatory submissions. Due to inherent differences in the data-generating mechanisms, biases can arise. This paper aims to illustrate how to use quantitative bias analysis (QBA). Advanced non-small cell lung cancer (NSCLC) serves as an example, where many small subsets of patients with molecular tumor subtypes exist. First, some sources of bias that may occur in oncology when comparing RWD to SAT are described. Second, using a hypothetical immunotherapy agent, a dataset is simulated based on expert input for survival analysis of advanced NSCLC. Finally, we illustrate the impact of three biases: missing confounder, misclassification of exposure, and outcome evaluation. For each simulated scenario, bias was induced by removing or adding data; hazard ratios (HRs) were estimated applying conventional analyses. Estimating the bias-adjusted treatment effect and uncertainty required carefully selecting the bias model and bias factors. Although the magnitude of each biased and bias-adjusted HR appeared moderate in all three hypothetical scenarios, the direction of bias was variable. These findings suggest that QBA can provide an intuitive framework for bias analysis, providing a key means of challenging assumptions about the evidence. However, the accuracy of bias analysis is itself dependent on correct specification of the bias model and bias factors. Ultimately, study design should reduce bias, but QBA allows us to evaluate the impact of unavoidable bias to assess the quality of the evidence.",2024,,Pharmacoepidemiology and drug safety,,,,38680093,10.1002/pds.5796,,#2006,ChristenGray 2024,"",""
A Two-Step Framework for Validating Causal Effect Estimates.,"Lingjie Shen, Erik Visser, Felice van Erning, Gijs Geleijnse, Maurits Kaptein","Comparing causal effect estimates obtained using observational data to those obtained from the gold standard (i.e., randomized controlled trials [RCTs]) helps assess the validity of these estimates. However, comparisons are challenging due to differences between observational data and RCT generated data. The unknown treatment assignment mechanism in the observational data and varying sampling mechanisms between the RCT and the observational data can lead to confounding and sampling bias, respectively. The objective of this study is to&#xa0;propose a two-step framework to validate causal effect estimates obtained from observational data by adjusting for both mechanisms. An estimator of causal effects related to the two mechanisms is constructed. A two-step framework for comparing causal effect estimates is derived from the estimator. An R package RCTrep is developed to implement the framework in practice. A simulation study is conducted to show that using our framework observational data can produce causal effect estimates similar to those of an RCT. A real-world application of the framework to validate treatment effects of adjuvant chemotherapy obtained from registry data is demonstrated. This &#xa0;study constructs a framework for comparing causal effect estimates between observational data and RCT data, facilitating the assessment of the validity of causal effect estimates obtained from observational data.",2024,,Pharmacoepidemiology and drug safety,,,,39252380,10.1002/pds.5873,,#2076,LingjieShen 2024,"",""
Validation Assessment of Privacy-Preserving Synthetic Electronic Health Record Data: Comparison of Original Versus Synthetic Data on Real-World COVID-19 Vaccine Effectiveness.,"Echo Wang, Katrina Mott, Hongtao Zhang, Sivan Gazit, Gabriel Chodick, Mehmet Burcu","To assess the validity of privacy-preserving synthetic data by comparing results from synthetic versus original EHR data analysis. A published retrospective cohort study on real-world effectiveness of COVID-19 vaccines by Maccabi Healthcare Services in Israel was replicated using synthetic data generated from the same source, and the results were compared between synthetic versus original datasets. The endpoints included COVID-19 infection, symptomatic COVID-19 infection and hospitalization due to infection and were also assessed in several demographic and clinical subgroups. In comparing synthetic versus original data estimates, several metrices were utilized: standardized mean differences (SMD), decision agreement, estimate agreement, confidence interval overlap, and Wald test. Synthetic data were generated five times to assess the stability of results. The distribution of demographic and clinical characteristics demonstrated very small difference (&lt;&#x2009;0.01 SMD). In the comparison of vaccine effectiveness assessed in relative risk reduction between synthetic versus original data, there was a 100% decision agreement, 100% estimate agreement, and a high level of confidence interval overlap (88.7%-99.7%) in all five replicates across all subgroups. Similar findings were achieved in the assessment of vaccine effectiveness against symptomatic COVID-19 Infection. In the comparison of hazard ratios for COVID 19-related hospitalization and odds ratio for symptomatic COVID-19 Infection, the Wald tests suggested no significant difference between respective effect estimates in all five replicates for all patient subgroups but there were disagreements in estimate and decision metrices in some subgroups and replicates. Overall, comparison of synthetic versus original real-world data demonstrated good validity and reliability. Transparency on the process to generate high fidelity synthetic data and assurances of patient privacy are warranted.",2024,,Pharmacoepidemiology and drug safety,,,,39375947,10.1002/pds.70019,,#2086,EchoWang 2024,"",""
Alternative analytic and matching approaches for the prevalent new-user design: a simulation study.,Webster-Clark M; Mavros P; Garry Em; Stürmer T; Shmuel S; Young J; Girman C,"To describe the creation of prevalent new user (pnu) cohorts and compare the relative bias and computational efficiency of several alternative analytic and matching approaches in pnu studies. In a simulated cohort, we estimated the effect of a treatment of interest vs a comparator among those who switched to the treatment of interest using the originally proposed time-conditional propensity score (tcps) matching, standardized morbidity ratio weighting (smrw), disease risk scores (drs), and several alternative propensity score matching approaches. For each analytic method, we compared the average rr (across 2000 replicates) to the known risk ratio (rr) of 1.00. Smrw and drs yielded unbiased results (rr = 0.998 and 0.997, respectively). Tcps matching with replacement was also unbiased (rr = 0.999). Tcps matching without replacement was unbiased when matches were identified starting with patients with the shortest treatment history as initially proposed (rr = 0.999), but it resulted in very slight bias (rr = 0.983) when starting with patients with the longest treatment history. Similarly, creating a match pool without replacement starting with patients with the shortest treatment history yielded an unbiased estimate (rr = 0.997), but matching with the longest treatment history first resulted in substantial bias (rr = 0.903). The most biased strategy was matching after selecting one random comparator observation per individual that continued on the comparator (rr = 0.802). Multiple analytic methods can estimate treatment effects without bias in a pnu cohort. Still, researchers should be wary of introducing bias when selecting controls for complex matching strategies beyond the initially proposed tcps.",,,,,,,35505471,10.1002/pds.5446,,#2107,,"",""
Sampling in the case-time-control design among drug users when outcome prevents further treatment.,Madsen Jeh; Hallas J; Delvin T; Scheike T; Pipper C,"The objective of this article is to advocate a new way of sampling controls in the case-time-control design in a cohort of drug users when the studied outcome prevents further treatment. Mathematically we demonstrate how a standard sampling of controls, where controls are sampled among all subjects without an event at end-of-study, leads to a biased effect estimate. We propose to add the requirement that controls initiate treatment before the calendar time of event of their matched case to circumvent this. The standard and proposed sampling methods are compared in a simulation study and in an empirical data example examining the effect of nonsteroidal anti-inflammatory drug usage on the risk of upper gastrointestinal bleeding. When the controls are sampled the standard way, the case-time-control design confers a bias because cases and controls have a different time-trend of exposure. The bias has been upwards in all the scenarios we have investigated. The requirement we add to be a potential control ensures that cases and controls have the same time-trend of exposure when treatment and outcome are independent. The simulation study confirms that the proposed sampling method removes the bias between treatment and outcome. The proposed sampling method lowered the odds-ratio estimate from 3.72 to 3.26 in the data example. The proposed sampling method makes it possible to use the case-time-control design in a cohort of subjects with registered use of a drug when outcome prevents further treatment.",,,,,,,35088482,10.1002/pds.5410,,#2108,,"",""
A comparison of confounder selection and adjustment methods for estimating causal effects using large healthcare databases.,Benasseur I; Talbot D; Durand M; Holbrook A; Matteau A; Potter Bj; Renoux C; Schnitzer Me; Tarride Jé; Guertin Jr,"Confounding adjustment is required to estimate the effect of an exposure on an outcome in observational studies. However, variable selection and unmeasured confounding are particularly challenging when analyzing large healthcare data. Machine learning methods may help address these challenges. The objective was to evaluate the capacity of such methods to select confounders and reduce unmeasured confounding bias. A simulation study with known true effects was conducted. Completely synthetic and partially synthetic data incorporating real large healthcare data were generated. We compared bayesian adjustment for confounding (bac), generalized bayesian causal effect estimation (gbcee), group lasso and doubly robust estimation, high-dimensional propensity score (hdps), and scalable collaborative targeted maximum likelihood algorithms. For the hdps, two adjustment approaches targeting the effect in the whole population were considered: full matching and inverse probability weighting. In scenarios without hidden confounders, most methods were essentially unbiased. The bias and variance of the hdps varied considerably according to the number of variables selected by the algorithm. In scenarios with hidden confounders, substantial bias reduction was achieved by using machine-learning methods to identify proxies as compared to adjusting only by observed confounders. Hdps and group lasso performed poorly in the partially synthetic simulation. Bac, gbcee, and scalable collaborative-targeted maximum likelihood algorithms performed particularly well. Machine learning can help to identify measured confounders in large healthcare databases. They can also capitalize on proxies of unmeasured confounders to substantially reduce residual confounding bias.",,,,,,,34953160,10.1002/pds.5403,,#2109,,"",""
Pharmacological and epidemiological considerations while constructing treatment episodes using observational data: a simulation study.,Pazzagli L; Andersen M; Sessa M,"The permissible gap method is an extensively used approach for defining episodes of continuous treatment use in pharmacoepidemiology. This method uses the amount of drug redeemed, when available, and researcher-defined temporal gaps to fill the interval between the calculated end of coverage of a redeemed prescription and the date of redemption of the next prescription in the same treatment episode. The final scope is defining periods of continuous use of medications. There are strong pharmacological and epidemiological arguments for adding the gap at the end of each treatment episode. However, the evidence is scarce on the impact that such a practice has on measures of association. This study aims to compare the impact of adding or not adding the researcher-defined gap time to the end of a treatment episode on the incidence of drug discontinuation and the incidence rate for a simulated outcome that occurred during an observational window. Additionally, the study aims at assessing the magnitude of misclassification of exposure time between the two methods. A simulated dataset of 100 patients available in the r package adherer that contains 1080 redeemed prescriptions was used. A gap time of 90 days was used for constructing treatment episodes in an observational window of 365 days following the first redeemed prescription. Two approaches were used for defining treatment episodes that were named ""gap+"" and ""gap-"" and that respectively add and did not add the gap time at the end of a treatment episode. An outcome was simulated by using an exponential baseline hazard function with scale parameter λ = 0.5 and censoring at time t = 365 days. The incidence rate ratio for the simulated outcome between the two approaches was computed. The 100 patients were followed for a median time of 183 days (interquartile range, iqr 50-365 days) and a median time of 273 days (iqr 140-365 days), respectively using ""gap-"" and ""gap+"". During the first 100 days of the follow-up period, none of the patients was found to discontinue the treatment with the method ""gap+"" while 38 patients discontinued using the method ""gap-"". The approach ""gap+"" exerted a higher incidence rate for the simulated outcome among the exposed (0.98 events/person-years) when compared to the ""gap-"" (0.82 events/person-years) during defined periods of continuous treatment use. When comparing the two approaches and using the method ""gap-"" as the reference group, the incidence rate ratio for the simulated outcome was 1.20 (95% confidence interval: ci 0.72-2.02) among the exposed. This study showed that not adding the gap at the end of the treatment episodes leads to an overestimation of drug discontinuation, particularly at the beginning of the observational window, and an underestimation of the incidence rate of a hypothetical outcome during the period of exposure to the medication.",,,,,,,34611960,10.1002/pds.5366,,#2110,,"",""
Using multiple random index dates with the reverse waiting time distribution improves precision of estimated prescription durations.,Bødkergaard K; Selmer Rm; Hallas J; Kjerpeseth Lj; Skovlund E; Støvring H,"To improve the precision of prescription duration estimates when using the reverse waiting time distribution (rwtd). For each patient we uniformly sampled multiple random index dates within a sampling window of length   . For each index date, we identified the last preceding prescription redemption, if any, within distance   . Based on all pairs of last prescription and index date, we estimated prescription durations using the rwtd with robust variance estimation. In simulation studies with increasing misspecification we investigated bias, root mean square error (rmse) and coverage probability of the rwtd using multiple index dates (1, 5, 10, and 20). We applied the method to danish data on warfarin prescriptions from 2013 to 2014 stratifying by and adjusting for sex and age. In simulation scenarios without misspecification, the relative bias was negligible (-0.04% to 0.01%) and nominal coverage probabilities almost retained (93.8%-95.4%). Rmse decreased with the number of random index dates (e.g., from 1.3 with 1 index date to 0.6 days with 5). With misspecification, the relative bias was higher irrespective of the number of index dates. Precision increased with the number of index dates, and hence coverage probabilities decreased. When estimating durations of warfarin prescriptions in denmark, precision increased with number of index dates, in particular in strata with few patients (e.g., men 90+ years: width of 95% confidence interval was 16.2 days with 5 index dates versus 35.4 with 1). Increasing the number of random index dates used with the rwtd improved precision without affecting bias.",,,,,,,34382713,10.1002/pds.5340,,#2111,,"",""
Bias of time-varying exposure effects due to time-varying covariate measurement strategies.,Penning De Vries Bbl; Groenwold Rhh,"In studies of effects of time-varying drug exposures, adequate adjustment for time-varying covariates is often necessary to properly control for confounding. However, the granularity of the available covariate data may not be sufficiently fine, for example when covariates are measured for participants only when their exposure levels change. To illustrate the impact of choices regarding the frequency of measuring time-varying covariates, we simulated data for a large target trial and for large observational studies, varying in covariate measurement design. Covariates were measured never, on a fixed-interval basis, or each time the exposure level switched. For the analysis, it was assumed that covariates remain constant in periods of no measurement. Cumulative survival probabilities for continuous exposure and non-exposure were estimated using inverse probability weighting to adjust for time-varying confounding, with special emphasis on the difference between 5-year event risks. With monthly covariate measurements, estimates based on observational data coincided with trial-based estimates, with 5-year risk differences being zero. Without measurement of baseline or post-baseline covariates, this risk difference was estimated to be 49% based on the available observational data. With measurements on a fixed-interval basis only, 5-year risk differences deviated from the null, to 29% for 6-monthly measurements, and with magnitude increasing up to 35% as the interval length increased. Risk difference estimates diverged from the null to as low as -18% when covariates were measured depending on exposure level switching. Our simulations highlight the need for careful consideration of time-varying covariates in designing studies on time-varying exposures. We caution against implementing designs with long intervals between measurements. The maximum length required will depend on the rates at which treatments and covariates change, with higher rates requiring shorter measurement intervals.",,,,,,,34251702,10.1002/pds.5328,,#2112,,"",""
Differential frequency in imaging-based outcome measurement: bias in real-world oncology comparative-effectiveness studies.,Adamson Bjs; Ma X; Griffith Sd; Sweeney Em; Sarkar S; Bourla Ab,"Comparative-effectiveness studies using real-world data (rwd) can be susceptible to surveillance bias. In solid tumor oncology studies, analyses of endpoints such as progression-free survival (pfs) are based on progression events detected by imaging assessments. This study aimed to evaluate the potential bias introduced by differential imaging assessment frequency when using electronic health record (ehr)-derived data to investigate the comparative effectiveness of cancer therapies. Using a nationwide de-identified ehr-derived database, we first analyzed imaging assessment frequency patterns in patients diagnosed with advanced non-small cell lung cancer (ansclc). We used those rwd inputs to develop a discrete event simulation model of two treatments where disease progression was the outcome and pfs was the endpoint. Using this model, we induced bias with differential imaging assessment timing and quantified its effect on observed versus true treatment effectiveness. We assessed percent bias in the estimated hazard ratio (hr). The frequency of assessments differed by cancer treatment types. In simulated comparative-effectiveness studies, pfs hrs estimated using real-world imaging assessment frequencies differed from the true hr by less than 10% in all scenarios (range: 0.4% to -9.6%). The greatest risk of biased effect estimates was found comparing treatments with widely different imaging frequencies, most exaggerated in disease settings where time to progression is very short. This study provided evidence that the frequency of imaging assessments to detect disease progression can differ by treatment type in real-world patients with cancer and may induce some bias in comparative-effectiveness studies in some situations.",,,,,,,34227170,10.1002/pds.5323,,#2113,,"",""
Sensitivity analyses of unmeasured and partially-measured confounders using multiple imputation in a vaccine safety study.,Xu S; Clarke Cl; Newcomer Sr; Daley Mf; Glanz Jm,"Sensitivity analyses have played an important role in pharmacoepidemiology studies using electronic health records data. Despite the existence of quantitative bias analysis in pharmacoepidemiologic studies, simultaneously adjusting for unmeasured and partially measured confounders is challenging in vaccine safety studies. Our objective was to develop a flexible approach for conducting sensitivity analyses of unmeasured and partially-measured confounders concurrently for a vaccine safety study. We derived conditional probabilities for an unmeasured confounder based on bias parameters, used these conditional probabilities and monte carlo simulations to impute the unmeasured confounder, and re-constructed the analytic datasets as if the unmeasured confounder had been observed. We simultaneously imputed a partially measured confounder using a prediction model. We considered unmeasured breastfeeding and partially measured family history of type 1 diabetes (t1dm) in a study examining the association between exposure to rotavirus vaccination and t1dm. Before sensitivity analyses, the hazard ratios (hr) were 1.50 (95% ci, 0.81-2.77) for those partially exposed and 1.03 (95% ci, 0.62-1.72) for those fully exposed with unexposed children as the referent group. When breastfeeding and family history of t1dm were adjusted, the hr was 1.55 (95% ci, 0.84-2.87) for the partially exposed group; the hr was 0.98 (95% ci, 0.58-1.63) for the fully exposed group. We conclude that adjusting for unmeasured breastfeeding and partially measured family history of t1dm did not alter the conclusion that there was no evidence of association between rotavirus vaccination and developing t1dm. This novel approach allows for simultaneous adjustment for multiple unmeasured and partially-measured confounders.",,,,,,,33988275,10.1002/pds.5294,,#2114,,"",""
Greedy caliper propensity score matching can yield variable estimates of the treatment-outcome association-a simulation study.,Komen Jj; Belitser Sv; Wyss R; Schneeweiss S; Taams Ac; Pajouheshnia R; Forslund T; Klungel Oh,"Greedy caliper propensity score (ps) matching is dependent on randomness, which can ultimately affect causal estimates. We sought to investigate the variation introduced by this randomness. Based on a literature search to define the simulation parameters, we simulated 36 cohorts of different sizes, treatment prevalence, outcome prevalence, treatment-outcome-association. We performed 1:1 caliper and nearest neighbor (nn) caliper ps-matching and repeated this 1000 times in the same cohort, before calculating the treatment-outcome association. Repeating caliper and nn caliper matching in the same cohort yielded large variations in effect estimates, in all 36 scenarios, with both types of matching. The largest variation was found in smaller cohorts, where the odds ratio (or) ranged from 0.53 to 10.00 (iqr of ors: 1.11-1.67). The 95% confidence interval was not consistently overlapping a neutral association after repeating the matching with both algorithms. We confirmed these findings in a noninterventional example study. Caliper ps-matching can yield highly variable estimates of the treatment-outcome association if the analysis is repeated.",,,,,,,33733533,10.1002/pds.5232,,#2115,,"",""
Machine learning outcome regression improves doubly robust estimation of average causal effects.,Choi By; Wang Cp; Gelfond J,"Doubly robust estimation produces an unbiased estimator for the average treatment effect unless both propensity score (ps) and outcome models are incorrectly specified. Studies have shown that the doubly robust estimator is subject to more bias than the standard weighting estimator when both ps and outcome models are incorrectly specified. We evaluated whether various machine learning methods can be used for estimating conditional means of the potential outcomes to enhance the robustness of the doubly robust estimator to various degrees of model misspecification in terms of reducing bias and standard error. We considered four types of methods to predict the outcomes: least squares, tree-based methods, generalized additive models and shrinkage methods. We also considered an ensemble method called the super learner (sl), which is a linear combination of multiple learners. We conducted simulations considering different scenarios by the complexity of ps and outcome-generating models and some ranges of treatment prevalence. The shrinkage methods performed well with robust doubly robust estimates in term of bias and mean squared error across the scenarios when the models became rich by including all 2-way interactions of the covariates. The sl performed similarly to the best method in each scenario. Our findings indicate that machine learning methods such as the sl or the shrinkage methods using interaction models should be used for more accurate doubly robust estimators.",,,,,,,32716126,10.1002/pds.5074,,#2117,,"",""
Bias in case-crossover studies of medications due to persistent use: a simulation study.,Bykov K; Wang Sv; Hallas J; Pottegård A; Maclure M; Gagne Jj,"The case-crossover design is increasingly used to evaluate the effects of chronic medications; however, as traditionally implemented in pharmacoepidemiology, with referent period preceding the outcome, it may lead to bias in the presence of persistent exposures. We aimed to evaluate the extent and magnitude of bias in case-crossover analyses of chronic and persistent exposures, using simulations. We simulated cohorts with either 30-day, 180-day, or 2-year exposure duration; and with varying degrees of persistence (10%, 30%, 50%, 70%, or 90% of patients not stopping exposure). We evaluated all scenarios under the null and the scenario with 30% persistence under varying exposure effects (odds ratios of 0.25 to 4.0). Cohorts were analyzed using conditional logistic regression that compared the odds of exposure on the outcome day to the odds of exposure on a referent day 30 days prior to the outcome. We further implemented the case-time-control design to evaluate its ability to adjust for bias from persistence. Case-crossover analyses produced unbiased estimates across all scenarios without persistent users, regardless of exposure duration. In scenarios where some patients persisted on treatment, case-crossover analyses resulted in upward bias, which increased with increasing proportion of persistent users, but did not vary substantially in relation to the magnitude of the true effect. Case-time-control analyses removed bias in all scenarios. Investigators should be aware of bias due to treatment persistence in unidirectional case-crossover analyses of chronic medications, which can be remedied with a control group of similarly persistent noncases.",,,,,,,32548875,10.1002/pds.5031,,#2118,,"",""
Using the waiting time distribution with random index dates to estimate prescription durations in the presence of seasonal stockpiling.,Bødkergaard K; Selmer Rm; Hallas J; Kjerpeseth Lj; Pottegård A; Skovlund E; Støvring H,"A pervasive problem in registry-based pharmacoepidemiological studies is what exposure duration to assign to individual prescriptions. The parametric waiting time distribution (wtd) has been proposed as a method to estimate such durations. However, when prescription durations vary due to seasonal stockpiling, wtd estimates will vary with choice of index date. To counter this, we propose using random index dates. Within a calendar period of a given length, δ, we randomly sample individual index dates. We include the last prescription redemption prior to the index date in the analysis. Only redemptions within distance δ of the index date are included. In a simulation study with varying types and degrees of stockpiling at the end of the year, we investigated bias and precision of the reverse wtd with fixed and random index dates, respectively. In addition, we applied the new method to estimate durations of norwegian warfarin prescriptions in 2014. In simulation settings with stockpiling, the reverse wtd with random index dates had low relative biases (-0.65% to 6.64%) and high coverage probabilities (92.0% to 95.3%), although when stockpiling was pronounced, coverage probabilities decreased (2.7% to 85.8%). Using a fixed index date was inferior. The estimated duration of warfarin prescriptions in norway using random index dates was 131 (130; 132) days. In the presence of seasonal stockpiling, the wtd with random index dates provides estimates of prescription durations, which are more stable, less biased and with better coverage when compared to using a fixed index date.",,,,,,,32436295,10.1002/pds.5026,,#2119,,"",""
Adverse drug reaction or innocent bystander? a systematic comparison of statistical discovery methods for spontaneous reporting systems.,Dijkstra L; Garling M; Foraita R; Pigeot I,"Spontaneous reporting systems (srss) are used to discover previously unknown relationships between drugs and adverse drug reactions (adrs). A plethora of statistical methods have been proposed over the years to identify these drug-adr pairs. The objective of this study is to compare a wide variety of methods in their ability to detect these signals, especially when their detection is complicated by the presence of innocent bystanders (drugs that are mistaken to be associated with the adr, since they are prescribed together with the drug that is the adr's actual cause). Twelve methods, 24 measures in total, ranging from simple disproportionality measures (eg, the reporting odds ratio), hypothesis tests (eg, test of the poisson mean), bayesian shrinkage estimates (eg, the bayesian confidence propagation neural network, bcpnn) to sparse regression (lasso), are compared in their ability to detect drug-adr pairs in a large number of simulated srss with varying numbers of innocent bystanders and effect sizes. The area under the precision-recall curve is used to assess the measures' performance. Hypothesis tests (especially the test of the poisson mean) perform best when the associations are weak and there is little to no confounding by other drugs. When the level of confounding increases and/or the effect sizes become larger, bayesian shrinkage methods should be preferred. The lasso proves to be the most robust against the innocent bystander effect. There is no absolute ""winner"". Which method to use for a particular srs depends on the effect sizes and the level of confounding present in the data.",,,,,,,32092786,10.1002/pds.4970,,#2122,,"",""
Improving measurement of binary covariates in claims data: a simulation study.,Connolly Jg; Glynn Rj; Schneeweiss S; Gagne Jj,"When investigators have two claims-based definitions for a binary confounder, it is unclear whether to prefer the more sensitive or more specific definition. Our objective was to compare adjusting for the sensitive or specific definition alone vs two novel approaches combining both definitions: a ""two-algorithm indicator"" and a ""two-algorithm restriction"" approach. Each simulated patient had a binary exposure, outcome, and confounder. We created two nested, misclassified versions of the confounder using validated heart failure definitions. The sensitive definition had a sensitivity/specificity of 0.98/0.83, while the specific definition had a sensitivity/specificity of 0.77/0.99. Patients were classified into 3 groups: group 0 did not meet either definition, group 1 met the sensitive but not specific definition, and group 2 met both. The two-algorithm indicator approach adjusted using indicators for groups 1 and 2, while the two-algorithm restriction approach excluded patients in group 1 and adjusted using an indicator for group 2. Adjusted exposure odds ratios (ors) were estimated for each approach using logistic regression. The crude or was 1.33 (95% ci, 1.07-1.63). Adjusting for the specific or sensitive definitions resulted in ors of 1.09 (95% ci, 0.87-1.35) and 1.14 (95% ci, 0.91-1.40). The two-algorithm indicator method returned an or of 1.07 (95% ci, 0.86-1.33). The two-algorithm restriction approach returned an or of 1.02 (95% ci, 0.79-1.29) but excluded 20% of the cohort. The two-algorithm indicator approach may improve adjustment for claims-based confounders by returning a point estimate at least as unbiased as the better of the two component definitions.",,,,,,,31972062,10.1002/pds.4961,,#2123,,"",""
Multiple imputation for systematically missing confounders within a distributed data drug safety network: a simulation study and real-world example.,Secrest Mh; Platt Rw; Reynier P; Dormuth Cr; Benedetti A; Filion Kb,"In distributed data networks, some data sites may be systematically missing important confounders that are captured by other sites in the network (eg, body mass index [bmi]). Multiple imputation may help repair bias in these scenarios. However, multiple imputation has not been described for distributed data networks where data access restrictions prevent centralized analysis. We conducted a simulation study and a real-world analysis using the uk's clinical practice research datalink to evaluate multiple imputation for confounders that are systematically missing from a subset of data sites in mock distributed data networks. The simulation study addressed univariate missing data, while the real-world analysis addressed multivariate missing data. Both studies were designed as retrospective cohort studies of the effect of current statin use on the risk of myocardial infarction among patients with newly treated type 2 diabetes. In our simulation study, multiple imputation repaired bias from missing bmi in all scenarios, with a median bias reduction of 118% in the default scenario. In our real-world study, the multiply imputed analysis (hazard ratio [hr]: 0.86; 95% confidence interval [ci], 0.69-1.08) was closer to the analysis that considered the true confounder values (hr: 0.85; 95% ci, 0.66-1.10) than the analysis that ignored them (hr: 0.93; 95% ci, 0.73-1.20). Multiple imputation adapted to distributed data settings is a feasible method to reduce bias from unmeasured but measurable confounders when at least one database contains the variables of interest. Further research is needed to evaluate its validity in real distributed data networks.",,,,,,,31486165,10.1002/pds.4876,,#2124,,"",""
Comparison of alternative approaches to trim subjects in the tails of the propensity score distribution.,Glynn Rj; Lunt M; Rothman Kj; Poole C; Schneeweiss S; Stürmer T,"In nonexperimental comparative effectiveness research, restricting analysis to subjects with better overlap of covariate distributions, hence greater treatment equipoise, helps balance the groups compared and can improve validity. Three alternative approaches, derived from different perspectives, implement restriction by trimming observations in the tails of the propensity score (ps). Across approaches, we compared the relationships between the overlap in treatment-specific ps distributions and the size of the balanced study population after trimming. The three trimming approaches considered were absolute trimming to the range 0.1<ps<0.9, asymmetric trimming to include subjects in both treatment groups with ps above the 5th percentile of the distribution in the target group and below the 95th percentile in the comparison group, and restriction to preference score values between 0.3 and 0.7. Comparisons of approaches used simulated pss from beta distributions and two example studies. The magnitude of the c-statistic strongly predicted (r  ≥.95) the percent of the balanced study population remaining. The balanced study population was largest under trimming at absolute ps levels, unless the target treatment was uncommon. Fewer than half of original study subjects remained after preference score trimming if c≥.80 and after asymmetric trimming if c≥.85. In examples, trimming improved the precision of estimated risk differences and identified apparent treatment effect heterogeneity in the ps tails where covariate balance was limited. Relative amounts of trimming in examples reflected the simulation results. Study populations with high ps c-statistics include only small percentages of subjects in whom valid treatment effects are confidently expected.",,,,,,,31385394,10.1002/pds.4846,,#2125,,"",""
A simulation study of the statistical power and signaling characteristics of an early season sequential test for influenza vaccine safety.,Forshee Ra; Hu M; Arya D; Perez-Vilar S; Anderson Sa; Lo Ac; Swarr M; Wernecke M; Macurdy T; Chu S; Kelman J,"The us food and drug administration monitors the risk of guillain-barré syndrome (gbs) following influenza vaccination using several data sources including medicare. In the 2017 to 2018 season, we transitioned our near real-time surveillance in medicare to more effectively detect large gbs risk increases early in the season while avoiding false positives. We conducted a simulation study examining the ability of the updating sequential probability ratio test (usprt) to detect substantially elevated gbs risk in the 8- to 21-day postvaccination versus 5× to 30× the historical rate. We varied the first testing week (weeks 5-8) and the null rate (1×-3×) and evaluated power. We estimated signal probability and the risk ratio (rr) after signaling when high-risk seasons were rare. Applying fixed alternatives, we found >80% power to detect a risk 30× the historical rate in week 5 for the 1× null and in week 6 for the 1.5× to 3× nulls. Nearly all testing schedules had >80% power for a 5× risk by week 11. To test the robustness of usprt, we further simulated seasons where 1% were true high-risk seasons. Using a 1× null led to 10% of seasons signaling by week 11 (median rr approximately 1.4), which decreased to approximately 1% with the ≥2.5× null (median rr approximately 16.0). On the basis of the results from this simulation and subsequent consultations with experts and stakeholders, we specified usprt to test continuously from weeks 7 to 11 using the null hypothesis that the observed gbs rate was 2.5× the historical rate. This helped improve the ability of usprt to provide early detection of gbs risk following influenza vaccination as part of a multilayered system of surveillance.",,,,,,,31222967,10.1002/pds.4807,,#2126,,"",""
A tool for empirical equipoise assessment in multigroup comparative effectiveness research.,Yoshida K; Solomon Dh; Haneuse S; Kim Sc; Patorno E; Tedeschi Sk; Lyu H; Hernández-Díaz S; Glynn Rj,"In observational research, equipoise concerns whether groups being compared are similar enough for valid inference. Empirical equipoise was previously proposed as a tool to assess patient similarity based on propensity scores (ps). We extended this work for multigroup observational studies. We modified the tool to allow for multinomial exposures such that the proposed definition reduces to the original when there are only two groups. We illustrated how the tool can be used as a method to assess study design within three-group clinical examples. We then conducted three-group simulations to assess how the tool performed in a setting with residual confounding after ps weighting. In a clinical example based on rheumatoid arthritis, 44.5% of the sample fell within the region of empirical equipoise when considering first-line biologics, whereas 57.7% did so for second-line biologics, consistent with the expectation that a second-line design results in better equipoise. In a simulation where the unmeasured confounder had the same magnitude of association with the treatment as the measured confounders and a 25% greater association with the outcome, the tool crossed the proposed threshold for empirical equipoise at a residual confounding of 20% on the ratio scale. When the unmeasured variable had a twice larger association with treatment, the tool became less sensitive and crossed the threshold at a residual confounding of 30%. Our proposed tool may be useful in guiding cohort identification in multigroup observational studies, particularly with similar effects of unmeasured and measured covariates on treatment and outcome.",,,,,,,31131965,10.1002/pds.4767,,#2128,,"",""
Evaluating the use of bootstrapping in cohort studies conducted with 1:1 propensity score matching-a plasmode simulation study.,Desai Rj; Wyss R; Abdia Y; Toh S; Johnson M; Lee H; Karami S; Major Jm; Nguyen M; Wang Sv; Franklin Jm; Gagne Jj,"Bootstrapping can account for uncertainty in propensity score (ps) estimation and matching processes in 1:1 ps-matched cohort studies. While theory suggests that the classical bootstrap can fail to produce proper coverage, practical impact of this theoretical limitation in settings typical to pharmacoepidemiology is not well studied. In a plasmode-based simulation study, we compared performance of the standard parametric approach, which ignores uncertainty in ps estimation and matching, with two bootstrapping methods. The first method only accounted for uncertainty introduced during the matching process (the observation resampling approach). The second method accounted for uncertainty introduced during both ps estimation and matching processes (the ps reestimation approach). Variance was estimated based on percentile and empirical standard errors, and treatment effect estimation was based on median and mean of the estimated treatment effects across 1000 bootstrap resamples. Two treatment prevalence scenarios (5% and 29%) across two treatment effect scenarios (hazard ratio of 1.0 and 2.0) were evaluated in 500 simulated cohorts of 10 000 patients each. We observed that 95% confidence intervals from the bootstrapping approaches but not the standard approach, resulted in inaccurate coverage rates (98%-100% for the observation resampling approach, 99%-100% for the ps reestimation approach, and 95%-96% for standard approach). Treatment effect estimation based on bootstrapping approaches resulted in lower bias than the standard approach (less than 1.4% vs 4.1%) at 5% treatment prevalence; however, the performance was equivalent at 29% treatment prevalence. Use of bootstrapping led to variance overestimation and inconsistent coverage, while coverage remained more consistent with parametric estimation.",,,,,,,31020732,10.1002/pds.4784,,#2129,,"",""
An evaluation of the impact of missing deaths on overall survival analyses of advanced non-small cell lung cancer patients conducted in an electronic health records database.,Carrigan G; Whipple S; Taylor Md; Torres Az; Gossai A; Arnieri B; Tucker M; Hofmeister Pp; Lambert P; Griffith Sd; Capra Wb,"The aim of this study was to assess the impact of missing death data on survival analyses conducted in an oncology ehr-derived database. The study was conducted using the flatiron health oncology database and the national death index (ndi) as a gold standard. Three analytic frameworks were evaluated in advanced non-small cell lung cancer (ansclc) patients: median overall survival [mos]), relative risk estimates conducted within the ehr-derived database, and ""external control arm"" analyses comparing an experimental group augmented with mortality data from the gold standard to a control group from the ehr-derived database only. The hazard ratios (hrs) obtained within the ehr-derived database (91% sensitivity) and the external control arm analyses, were compared with results when both groups were augmented with mortality data from the gold standard. The above analyses were repeated using simulated lower mortality sensitivities to understand the impact of more extreme levels of missing deaths. Bias in mos ranged from modest (0.6-0.9 mos.) in the ehr-derived cohort with (91% sensitivity) to substantial when lower sensitivities were generated through simulation (3.3-9.7 mos.). Overall, small differences were observed in the hrs for the ehr-derived cohort across comparative analyses when compared with hrs obtained using the gold standard data source. When only one treatment arm was subject to estimation bias, the bias was slightly more pronounced, but increased substantially when lower sensitivities were simulated. The impact on survival analysis is minimal with high mortality sensitivity with only modest impact associated within external control arm applications.",,,,,,,30873729,10.1002/pds.4758,,#2130,,"",""
Quantifying bias reduction with fixed-duration versus all-available covariate assessment periods.,Connolly Jg; Schneeweiss S; Glynn Rj; Gagne Jj,"Implementing a cohort study in longitudinal healthcare databases requires looking back over some covariate assessment period (cap) preceding cohort entry to measure confounders. We used simulations to compare fixed-duration versus all-available caps for confounder adjustment in the presence of differences in available baseline time between exposure groups. We simulated cohorts of 10 000 patients with binary variables for a single confounder, exposure, and outcome. Baseline time was simulated based on the observed distribution in a claims-based comparison of statin users versus nonusers. We compared bias after measuring confounders using fixed-duration and all-available caps, both when exposure groups had similar and discrepant amounts of available baseline time. When the comparison groups had similar amounts of baseline time, an all-available cap was less biased than a fixed-duration cap. When baseline time differed between comparison groups, the preferable cap approach depended on the direction of confounding and which exposure group had higher covariate sensitivity. These findings were consistent in direction across sensitivity analyses. In certain settings of differential available baseline time between exposure groups, the all-available cap was more biased than the fixed-duration cap. The relative directions and strengths of confounding and misclassification biases are an important consideration when choosing between a fixed-duration or all-available cap, but they are often unknown. Therefore, we recommend comparing the amount of available baseline time between exposure groups. When there is a large discrepancy, despite appropriate design choices, we recommend a fixed-duration approach to avoid potential increases in bias because of differential data availability.",,,,,,,30786103,10.1002/pds.4729,,#2131,,"",""
Propensity score methods and regression adjustment for analysis of nonrandomized studies with health-related quality of life outcomes.,Cottone F; Anota A; Bonnetain F; Collins Gs; Efficace F,"The aim of this study was to investigate the potential added value of combining propensity score (ps) methods with multivariable linear regression (mlr) in estimating the average treatment effect on the treated (att) in nonrandomized studies with health-related quality of life (hrqol) outcomes. We first used simulations to compare the performances of different ps-based methods, either alone or in combination with further mlr adjustment, in estimating att. Ps methods were, respectively, optimal pair (opm) and full (ofm) ps matching, subclassification on the ps (sbc), and the inverse probability of treatment weighting (iptw). We simulated several scenarios, according to different sample sizes, proportions of treated vs untreated subjects, and types of hrqol outcomes. We also applied the same methods to a real clinical data set. Opm and iptw provided the closest type i error to the nominal threshold α = 0.05 across all scenarios. Overall, both methods showed also lower variability in estimates than sbc and ofm. Sbc performed worst, generally providing the highest levels of bias. Further mlr adjustment lessened bias for all methods, however providing higher type i error for sbc and ofm. In the real case, all methods provided similar att estimates except for one outcome. Our findings suggest that for sample sizes up to n = 200, opm and iptw are to be preferred to ofm and sbc in estimating att on hrqol outcomes. Specifically, opm performed best in sample sizes of n ≥ 80, iptw for smaller sample sizes. Additional mlr adjustment can further improve att estimates.",,,,,,,30784132,10.1002/pds.4756,,#2132,,"",""
Conditional validation sampling for consistent risk estimation with binary outcome data subject to misclassification.,Gravel Ca; Farrell Pj; Krewski D,"Misclassification of a binary outcome can introduce bias in estimation of the odds-ratio associated with an exposure of interest in pharmacoepidemiology research. It has been previously demonstrated that utilizing information from an internal randomly selected validation sample can help mitigate this bias. Using a monte carlo simulation-based approach, we study the properties of misclassification bias-adjusted odds-ratio estimators in a contingency table setting. We consider two methods of internal validation sampling; namely, simple random sampling and sampling conditional on the original (possibly incorrect) outcome status. Additional simulation studies are conducted to investigate these sampling approaches in a multi-table setting. We demonstrate that conditional validation sampling, across a range of subsampling fractions, can produce better estimates than those based on an unconditional simple random sample. This approach allows for greater flexibility in the chosen categorical composition of the validation data, as well as the potential for obtaining a more efficient estimator of the odds-ratio. We further demonstrate that this relationship holds for the mantel-haenszel misclassification bias-adjusted odds-ratio in stratified samples. Recommendations for the choice of validation subsampling fraction are also provided. Careful consideration when choosing the sampling scheme used to draw internal validation samples can improve the properties of the outcome misclassification bias-adjusted odds-ratio estimator in a (multiple) contingency table.",,,,,,,30746841,10.1002/pds.4701,,#2133,,"",""
Comparing external and internal validation methods in correcting outcome misclassification bias in logistic regression: a simulation study and application to the case of postsurgical venous thromboembolism following total hip and knee arthroplasty.,Ni J; Dasgupta K; Kahn Sr; Talbot D; Lefebvre G; Lix Lm; Berry G; Burman M; Dimentberg R; Laflamme Y; Cirkovic A; Rahme E,"We assessed the validity of postsurgery venous thromboembolism (vte) diagnoses identified from administrative databases and compared bayesian and multiple imputation (mi) approaches in correcting for outcome misclassification in logistic regression models. Sensitivity and specificity of postsurgery vte among patients undergoing total hip or knee replacement (thr/tkr) were assessed against chart review in six montreal hospitals in 2009 to 2010. Administrative data on all thr/tkr quebec patients in 2009 to 2010 were obtained. The performance of bayesian external, bayesian internal, and mi approaches to correct the odds ratio (or) of postsurgery vte in tertiary versus community hospitals was assessed using simulations. Bayesian external approach used prior information from external sources, while bayesian internal and mi approaches used chart review. In total, 17 319 patients were included, 2136 in participating hospitals, among whom 75 had vte in administrative data versus 81 in chart review. Vte sensitivity was 0.59 (95% confidence interval, 0.48-0.69) and specificity was 0.99 (0.98-0.99), overall. The adjusted or of vte in tertiary versus community hospitals was 1.35 (1.12-1.64) using administrative data, 1.45 (0.97-2.19) when mi was used for misclassification correction, and 1.53 (0.83-2.87) and 1.57 (0.39-5.24) when bayesian internal and external approaches were used, respectively. In simulations, all three approaches reduced the or bias and had appropriate coverage for both nondifferential and differential misclassification. Vte identified from administrative data had low sensitivity and high specificity. The bayesian external approach was useful to reduce outcome misclassification bias in logistic regression; however, it required accurate specification of the misclassification properties and should be used with caution.",,,,,,,30515908,10.1002/pds.4693,,#2134,,"",""
Inflation of type i error rates due to differential misclassification in ehr-derived outcomes: empirical illustration using breast cancer recurrence.,Chen Y; Wang J; Chubak J; Hubbard Ra,"Many outcomes derived from electronic health records (ehr) not only are imperfect but also may suffer from exposure-dependent differential misclassification due to variability in the quality and availability of ehr data across exposure groups. The objective of this study was to quantify the inflation of type i error rates that can result from differential outcome misclassification. We used data on gold-standard and ehr-derived second breast cancers in a cohort of women with a prior breast cancer diagnosis from 1993 to 2006 enrolled in kaiser permanente washington. We simulated an exposure that was independent of the true outcome status. A surrogate outcome was then simulated with varying sensitivity and specificity according to exposure status. We estimated the type i error rate for a test of association relating this exposure to the surrogate outcome, while varying outcome sensitivity and specificity in exposed individuals. Type i error rates were substantially inflated above the nominal level (5%) for even modest departures from nondifferential misclassification. Holding sensitivity in exposed and unexposed groups at 85%, a difference in specificity of 10% between the exposed and unexposed (80% vs 90%) resulted in a 36% type i error rate. Type i error was inflated more by differential specificity than sensitivity. Differential outcome misclassification may induce spurious findings. Researchers using ehr-derived outcomes should use misclassification-adjusted methods whenever possible or conduct sensitivity analyses to investigate the possibility of false-positive findings, especially for exposures that may be related to the accuracy of outcome ascertainment.",,,,,,,30375122,10.1002/pds.4680,,#2135,,"",""
Comparison of privacy-protecting analytic and data-sharing methods: a simulation study.,Yoshida K; Gruber S; Fireman Bh; Toh S,"Privacy-protecting analytic and data-sharing methods that minimize the disclosure risk of sensitive information are increasingly important due to the growing interest in utilizing data across multiple sources. We conducted a simulation study to examine how avoiding sharing individual-level data in a distributed data network can affect analytic results. The base scenario had four sites of varying sizes with 5% outcome incidence, 50% treatment prevalence, and seven confounders. We varied treatment prevalence, outcome incidence, treatment effect, site size, number of sites, and covariate distribution. Confounding adjustment was conducted using propensity score or disease risk score. We compared analyses of three types of aggregate-level data requested from sites: risk-set, summary-table, or effect-estimate data (meta-analysis) with benchmark results of analysis of pooled individual-level data. We assessed bias and precision of hazard ratio estimates as well as the accuracy of standard error estimates. All the aggregate-level data-sharing approaches, regardless of confounding adjustment methods, successfully approximated pooled individual-level data analysis in most simulation scenarios. Meta-analysis showed minor bias when using inverse probability of treatment weights (iptw) in infrequent exposure (5%), rare outcome (0.01%), and small site (5,000 patients) settings. Se estimates became less accurate for iptw risk-set approach with less frequent exposure and for propensity score-matching meta-analysis approach with rare outcomes. Overall, we found that we can avoid sharing individual-level data and obtain valid results in many settings, although care must be taken with meta-analysis approach in infrequent exposure and rare outcome scenarios, particularly when confounding adjustment is performed with iptw.",,,,,,,30022561,10.1002/pds.4615,,#2136,,"",""
Bias from restricting to live births when estimating effects of prescription drug use on pregnancy complications: a simulation.,Suarez Ea; Landi Sn; Conover Mm; Jonsson Funk M,"Administrative claim databases are increasingly being used to study the safety of medication exposures during pregnancy. These studies are restricted to live births due to a reliance on algorithms for estimating gestational age that are based on codes associated with live delivery. Conditioning on live birth may induce selection bias when studying the effect of a drug on a pregnancy complication if fetal death is a competing risk for the complication or is caused by the complication. We simulated a population of 100,000 pregnancies and estimated the impact of selection bias on relative estimates for the effect of antidepressant exposure on the outcome of preeclampsia. We assumed that the exposure, outcome, and covariates increased the risk of fetal loss. A downward bias in the risk ratio was consistently observed when conditioning on live births. When an unmeasured covariate was assumed to be a common cause of fetal death, antidepressant use, and preeclampsia, the direction of bias varied depending on the strength of the confounding relationship coupled with the selection bias. Despite the very low prevalence of stillbirth, the strength of the relationship between antidepressant use and stillbirth had a substantial impact on bias. Conditioning on live birth can be problematic when studying pregnancy complications. Simple quantitative selection bias analysis in populations restricted to live births may not fully account for selection bias.",,,,,,,29341340,10.1002/pds.4387,,#2137,,"",""
Bias from outcome misclassification in immunization schedule safety research.,Newcomer Sr; Kulldorff M; Xu S; Daley Mf; Fireman B; Lewis E; Glanz Jm,"The institute of medicine recommended conducting observational studies of childhood immunization schedule safety. Such studies could be biased by outcome misclassification, leading to incorrect inferences. Using simulations, we evaluated (1) outcome positive predictive values (ppvs) as indicators of bias of an exposure-outcome association, and (2) quantitative bias analyses (qba) for bias correction. Simulations were conducted based on proposed or ongoing vaccine safety datalink studies. We simulated 4 studies of 2 exposure groups (children with no vaccines or on alternative schedules) and 2 baseline outcome levels (100 and 1000/100 000 person-years), with 3 relative risk (rr) levels (rr = 0.50, 1.00, and 2.00), across 1000 replications using probabilistic modeling. We quantified bias from non-differential and differential outcome misclassification, based on levels previously measured in database research (sensitivity > 95%; specificity > 99%). We calculated median outcome ppvs, median observed rrs, type 1 error, and bias-corrected rrs following qba. We observed ppvs from 34% to 98%. With non-differential misclassification and true rr = 2.00, median bias was toward the null, with severe bias (median observed rr = 1.33) with ppv = 34% and modest bias (median observed rr = 1.83) with ppv = 83%. With differential misclassification, ppvs did not reflect median bias, and there was type 1 error of 100% with ppv = 90%. Qba was generally effective in correcting misclassification bias. In immunization schedule studies, outcome misclassification may be non-differential or differential to exposure. Overall outcome ppvs do not reflect the distribution of false positives by exposure and are poor indicators of bias in individual studies. Our results support qba for immunization schedule safety research.",,,,,,,29292551,10.1002/pds.4374,,#2138,,"",""
Comparison of the ability of double-robust estimators to correct bias in propensity score matching analysis. A monte carlo simulation study.,Nguyen Tl; Collins Gs; Spence J; Devereaux Pj; Daurès Jp; Landais P; Le Manach Y,"As covariates are not always adequately balanced after propensity score matching and double- adjustment can be used to remove residual confounding, we compared the performance of several double-robust estimators in different scenarios. We conducted a series of monte carlo simulations on virtual observational studies. After estimating the propensity scores by logistic regression, we performed 1:1 optimal, nearest-neighbor, and caliper matching. We used 4 estimators on each matched sample: (1) a crude estimator without double-adjustment, (2) double-adjustment for the propensity scores, (3) double-adjustment for the unweighted unbalanced covariates, and (4) double-adjustment for the unbalanced covariates, weighted by their strength of association with the outcome. The crude estimator led to highest bias in all tested scenarios. Double-adjustment for the propensity scores effectively removed confounding only when the propensity score models were correctly specified. Double-adjustment for the unbalanced covariates was more robust to misspecification. Double-adjustment for the weighted unbalanced covariates outperformed the other approaches in every scenario and using any matching algorithm, as measured by the mean squared error. Double-adjustment can be used to remove residual confounding after propensity score matching. The unbalanced covariates with the strongest confounding effects should be adjusted.",,,,,,,28984050,10.1002/pds.4325,,#2139,,"",""
Combining evidence from multiple electronic health care databases: performances of one-stage and two-stage meta-analysis in matched case-control studies.,La Gamba F; Corrao G; Romio S; Sturkenboom M; Trifirò G; Schink T; De Ridder M,"Clustering of patients in databases is usually ignored in one-stage meta-analysis of multi-database studies using matched case-control data. The aim of this study was to compare bias and efficiency of such a one-stage meta-analysis with a two-stage meta-analysis. First, we compared the approaches by generating matched case-control data under 5 simulated scenarios, built by varying: (1) the exposure-outcome association; (2) its variability among databases; (3) the confounding strength of one covariate on this association; (4) its variability; and (5) the (heterogeneous) confounding strength of two covariates. Second, we made the same comparison using empirical data from the aritmo project, a multiple database study investigating the risk of ventricular arrhythmia following the use of medications with arrhythmogenic potential. In our study, we specifically investigated the effect of current use of promethazine. Bias increased for one-stage meta-analysis with increasing (1) between-database variance of exposure effect and (2) heterogeneous confounding generated by two covariates. The efficiency of one-stage meta-analysis was slightly lower than that of two-stage meta-analysis for the majority of investigated scenarios. Based on aritmo data, there were no evident differences between one-stage (or = 1.50, ci = [1.08; 2.08]) and two-stage (or = 1.55, ci = [1.12; 2.16]) approaches. When the effect of interest is heterogeneous, a one-stage meta-analysis ignoring clustering gives biased estimates. Two-stage meta-analysis generates estimates at least as accurate and precise as one-stage meta-analysis. However, in a study using small databases and rare exposures and/or outcomes, a correct one-stage meta-analysis becomes essential.",,,,,,,28799196,10.1002/pds.4280,,#2140,,"",""
Correcting hazard ratio estimates for outcome misclassification using multiple imputation with internal validation data.,Ni J; Leong A; Dasgupta K; Rahme E,"Outcome misclassification may occur in observational studies using administrative databases. We evaluated a two-step multiple imputation approach based on complementary internal validation data obtained from two subsamples of study participants to reduce bias in hazard ratio (hr) estimates in cox regressions. We illustrated this approach using data from a surveyed sample of 6247 individuals in a study of statin-diabetes association in quebec. We corrected diabetes status and onset assessed from health administrative data against self-reported diabetes and/or elevated fasting blood glucose (fbg) assessed in subsamples. The association between statin use and new onset diabetes was evaluated using administrative data and the corrected data. By simulation, we assessed the performance of this method varying the true hr, sensitivity, specificity, and the size of validation subsamples. The adjusted hr of new onset diabetes among statin users versus non-users was 1.61 (95% confidence interval: 1.09-2.38) using administrative data only, 1.49 (0.95-2.34) when diabetes status and onset were corrected based on self-report and undiagnosed diabetes (fbg ≥ 7 mmol/l), and 1.36 (0.92-2.01) when corrected for self-report and undiagnosed diabetes/impaired fbg (≥ 6 mmol/l). In simulations, the multiple imputation approach yielded less biased hr estimates and appropriate coverage for both non-differential and differential misclassification. Large variations in the corrected hr estimates were observed using validation subsamples with low participation proportion. The bias correction was sometimes outweighed by the uncertainty introduced by the unknown time of event occurrence. Multiple imputation is useful to correct for outcome misclassification in time-to-event analyses if complementary validation data are available from subsamples. Copyright © 2017 john wiley & sons, ltd.",,,,,,,28503870,10.1002/pds.4223,,#2141,,"",""
A comparison of entropy balance and probability weighting methods to generalize observational cohorts to a population: a simulation and empirical example.,Harvey Ra; Hayden Jd; Kamble Ps; Bouchard Jr; Huang Jc,"We compared methods to control bias and confounding in observational studies including inverse probability weighting (ipw) and stabilized ipw (sipw). These methods often require iteration and post-calibration to achieve covariate balance. In comparison, entropy balance (eb) optimizes covariate balance a priori by calibrating weights using the target's moments as constraints. We measured covariate balance empirically and by simulation by using absolute standardized mean difference (asmd), absolute bias (ab), and root mean square error (rmse), investigating two scenarios: the size of the observed (exposed) cohort exceeds the target (unexposed) cohort and vice versa. The empirical application weighted a commercial health plan cohort to a nationally representative national health and nutrition examination survey target on the same covariates and compared average total health care cost estimates across methods. Entropy balance alone achieved balance (asmd ≤ 0.10) on all covariates in simulation and empirically. In simulation scenario i, eb achieved the lowest ab and rmse (13.64, 31.19) compared with ipw (263.05, 263.99) and sipw (319.91, 320.71). In scenario ii, eb outperformed ipw and sipw with smaller ab and rmse. In scenarios i and ii, eb achieved the lowest mean estimate difference from the simulated population outcome ($490.05, $487.62) compared with ipw and sipw, respectively. Empirically, only eb differed from the unweighted mean cost indicating ipw, and sipw weighting was ineffective. Entropy balance demonstrated the bias-variance tradeoff achieving higher estimate accuracy, yet lower estimate precision, compared with ipw methods. Eb weighting required no post-processing and effectively mitigated observed bias and confounding. Copyright © 2016 john wiley & sons, ltd.",,,,,,,27859943,10.1002/pds.4121,,#2143,,"",""
